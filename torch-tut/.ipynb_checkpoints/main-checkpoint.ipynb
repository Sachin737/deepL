{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9165e-35,  4.5773e-41,  2.1484e-32,  0.0000e+00,  5.8294e-43],\n",
      "        [ 0.0000e+00,  1.8077e-43,  0.0000e+00,  2.1351e-32,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9487, 0.9078, 0.6132],\n",
      "        [0.8844, 0.0286, 0.9079],\n",
      "        [0.7871, 0.7995, 0.3456],\n",
      "        [0.3086, 0.6498, 0.7696],\n",
      "        [0.4874, 0.9135, 0.5395]])\n"
     ]
    }
   ],
   "source": [
    "x = t.rand(5, 3) # rand no. in [0,1]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([5, 3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(x.size(), x.shape, x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# datatype\n",
    "print(x.dtype)\n",
    "x = torch.ones(3,2,dtype=int)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.5000, 4.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# To calcualte gradient for this tensor\n",
    "x = torch.tensor([2, 3.5, 4], requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8710, 0.5985, 0.8928, 0.2475],\n",
      "        [0.1629, 0.9974, 0.1745, 0.8027],\n",
      "        [0.0611, 0.9410, 0.1655, 0.1645],\n",
      "        [0.1934, 0.1905, 0.6478, 0.8490],\n",
      "        [0.5332, 0.1547, 0.7727, 0.4118]])\n",
      "tensor([0.8710, 0.1629, 0.0611, 0.1934, 0.5332])\n"
     ]
    }
   ],
   "source": [
    "# slicing\n",
    "x = torch.rand(5,4)\n",
    "print(x)\n",
    "print(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3260, -2.4950, -0.3501,  0.5108],\n",
      "        [-0.3252,  1.6520, -0.6298, -0.7426],\n",
      "        [ 1.9315,  0.2365,  0.4863, -0.5565],\n",
      "        [-1.0657,  1.0835, -0.2443, -0.8860]]) \n",
      " tensor([[ 0.3260, -2.4950, -0.3501,  0.5108, -0.3252,  1.6520, -0.6298, -0.7426],\n",
      "        [ 1.9315,  0.2365,  0.4863, -0.5565, -1.0657,  1.0835, -0.2443, -0.8860]])\n"
     ]
    }
   ],
   "source": [
    "# reshape size\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(2, -1)\n",
    "print(x, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# TENSOR --> NUMPY ARRAY\n",
    "x = torch.ones(3,2)\n",
    "y = x.numpy()\n",
    "print(type(y))\n",
    "print(x)\n",
    "# *** If tensor is on GPU, then both tensor and np array will share \n",
    "#     same memory location, so changing one will change other too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]])\n",
      "[[3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "x.add_(2)\n",
    "print(x)\n",
    "print(y)  # <=== Both changed !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#  NUMPY ARRAY --> TENSOR\n",
    "a = np.ones(3)\n",
    "b = torch.from_numpy(a)  # <-- Share same mem locations\n",
    "c = torch.tensor(a) # <-- different memory\n",
    "print(type(a), type(b), type(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GPU Support\n",
    "##### By default Tensor proccesses on CPU, but we can allocate GPU to all tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "x = torch.rand(2,3).to(device)  # tensor moved to GPU device\n",
    "\n",
    "x = torch.rand(2,3, device=device) # created on GPU device only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "##### this package give automatic differentiation for all ops on tensor.\n",
    "##### It provides partial derivatives, while applying chain rule.\n",
    ">Set requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3744, -2.0941, -0.0507,  0.5229,  0.4360], requires_grad=True)\n",
      "tensor([ 1.6256, -0.0941,  1.9493,  2.5229,  2.4360], grad_fn=<AddBackward0>)\n",
      "tensor([3.6425, 1.0089, 4.7998, 7.3652, 6.9341], grad_fn=<AddBackward0>)\n",
      "tensor(4.7501, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "tensor([ 0.6502, -0.0376,  0.7797,  1.0092,  0.9744])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "\n",
    "y = x + 2\n",
    "z = y * y + 1\n",
    "w = z.mean()\n",
    "\n",
    "# All ops are tracked \n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(w)\n",
    "\n",
    "print(x.grad) \n",
    "w.backward() # <- backward propagation\n",
    "print(x.grad) #dz/dx\n",
    "\n",
    "# TO remove all gradient calcualted we use optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop a tensor from tracking history:\n",
    "##### For example during the training loop when we want to update our weights, or after training during evaluation. These operations should not be part of the gradient computation. To prevent this, we can use:\n",
    "- x.requires_grad_(False)\n",
    "- x.detach()\n",
    "- wrap in with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x7f9857ceb4f0>\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "x = torch.randn(4,2)\n",
    "y=x*x+3\n",
    "print(y.grad_fn)\n",
    "\n",
    "x.requires_grad_(True)\n",
    "y=x*x+3\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 2 : remove grad comp unit from tensor and create new tensor\n",
    "x = torch.randn(2,2, requires_grad=True)\n",
    "y = x.detach()\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x*x + x\n",
    "    print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "> f(x) = w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(-10) = -0.000\n",
      "epoch: 10  w: 1.9984041452407837  loss: 0.0002704716462176293\n",
      "epoch: 20  w: 1.999998688697815  loss: 1.7278267705478356e-10\n",
      "epoch: 30  w: 2.0  loss: 0.0\n",
      "epoch: 40  w: 2.0  loss: 0.0\n",
      "epoch: 50  w: 2.0  loss: 0.0\n",
      "epoch: 60  w: 2.0  loss: 0.0\n",
      "epoch: 70  w: 2.0  loss: 0.0\n",
      "epoch: 80  w: 2.0  loss: 0.0\n",
      "epoch: 90  w: 2.0  loss: 0.0\n",
      "epoch: 100  w: 2.0  loss: 0.0\n",
      "Prediction after training: f(-10) = -20.000\n"
     ]
    }
   ],
   "source": [
    "# here, fx = 2*x\n",
    "X = torch.tensor([1,2,3,4,5,6,7,8], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8,10,12,14,16], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "# model\n",
    "def forward(x):\n",
    "    return w*x\n",
    "def loss(y,y_):\n",
    "    return ((y_-y)**2).mean()\n",
    "\n",
    "X_test = -10\n",
    "print(f'Prediction before training: f({X_test}) = {forward(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "mu = 0.01\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Y_ = forward(X)\n",
    "    L = loss(Y,Y_)\n",
    "    \n",
    "    L.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= mu * w.grad\n",
    "        # !! dont do w = w - mu*w.grad\n",
    "        # It will create new w tensor, which will not be linked to any function history\n",
    "        # hence no grad for it.\n",
    "\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if(epoch+1) % 10 == 0:\n",
    "        print(\"epoch:\",epoch+1, \" w:\", w.item(), \" loss:\", L.item())\n",
    "\n",
    "# test\n",
    "print(f'Prediction after training: f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Loss & Optimizer\n",
    "\n",
    "A typical PyTorch pipeline:\n",
    "- Design model (input, output, fwd pass with diff layers)\n",
    "- Construct loss and optimizers\n",
    "- Training loop:\n",
    "    - Forward: compute pred and loss\n",
    "    - Backward: coputer grads\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameteres\n",
    "input_size = 784 #28*28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 24\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader : it helps in iterating over datasets\n",
    "train_loader = torch.utils.data.DataLoader(dataset=df_train,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=df_test,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGKCAYAAACsHiO8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvYElEQVR4nO3dfXRU1b3/8e8EyRAhmfAgCbmSEtQWllyxpgRTLEVJoWiRJ23xrlpoURQD9yIqlcqDostw4VZd0AjXPoDYi1BEoOBDFw0QFAOWgJcimqKCpA0JIGYmBpIg2b8/uMzPsHfKmcxkz5zJ+7XW+SOfnId9wjfJl5N9zvEopZQAAABYkhDtAQAAgLaF5gMAAFhF8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWNVqzUdhYaH06tVLOnToIAMHDpR33323tQ4FRBS1C7eiduEWntZ4t8uaNWvkJz/5iSxbtkwGDhwozz33nKxdu1bKysqke/fu/3TbxsZGqaiokOTkZPF4PJEeGtoIpZTU1NRIRkaGJCQ477GpXUQbtQu3Cql2VSvIyclR+fn5wY/PnTunMjIyVEFBwSW3LS8vVyLCwhKRpby8nNplceVC7bK4dXFSuxH/s0tDQ4OUlpZKXl5eMEtISJC8vDwpKSnR1q+vr5dAIBBcFC/ZRQQlJyc7XpfaRSyhduFWTmo34s3HyZMn5dy5c5KWltYkT0tLk8rKSm39goIC8fl8wSUzMzPSQ0IbFsolZGoXsYTahVs5qd2o3+0ya9Ys8fv9waW8vDzaQwIcoXbhVtQuou2ySO+wW7du0q5dO6mqqmqSV1VVSXp6ura+1+sVr9cb6WEAIaN24VbULtwm4lc+EhMTJTs7W4qKioJZY2OjFBUVSW5ubqQPB0QMtQu3onbhOiFNp3Zo9erVyuv1qhUrVqiDBw+qyZMnq9TUVFVZWXnJbf1+f9Rn6rLEz+L3+6ldFlcu1C6LWxcntdsqzYdSSi1ZskRlZmaqxMRElZOTo3bt2uVoO74JWCK5hPoDnNpliZWF2mVx6+KkdlvlIWPhCAQC4vP5oj0MxAm/3y8pKSlWjkXtIpKoXbiVk9qN+t0uAACgbaH5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVl0V7APHm2muvdbTexIkTjXmHDh0cbZ+fn69lGzZs0LI//vGPxu1fffVVLQsEAo6ODQBAOLjyAQAArKL5AAAAVtF8AAAAq2g+AACAVUw4daB79+7GfOnSpVo2evRoLVNKRXpIxn3efvvtjjIRkfHjx2vZD3/4Qy1jEiqAWNKxY0ctS05O1rIRI0ZoWd++fR0fZ8CAAVo2ZMgQLdu8ebOWffDBB8Z9rly5UstOnjypZZWVlQ5G6G5c+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqPao3ZkGEIBALi8/midvxu3bpp2euvv25c94YbbtAyj8ejZZ9//rmWnT171rjPiooKLdu4caOj45gml15zzTXG41x++eVa9tZbb2nZzTffbNzeLfx+v6SkpFg5VrRrF/GF2jV7+OGHtew///M/tSyUX22mn9Hl5eVaZvq5azpO7969jcfp1KmTo/GMHDlSy9544w1H28YCJ7XLlQ8AAGAVzQcAALCK5gMAAFhF8wEAAKziCacXMb2q3jSxtDlPPvmklhUWFmrZiRMnQhuYA0888YSW3XvvvcZ1TU9n/c53vhPxMSG6Hn/8cWOelZXlaHvTxLmbbrrJuG44c9edTuQTEVm4cKGWLVq0SMs+++yzFo8HsevIkSNa9umnn2pZZmamlv3iF78w7tN0U8GBAwdCH9z/ae53xvTp07Xsjjvu0LJrr71Wy9w04dQJrnwAAACraD4AAIBVNB8AAMAqmg8AAGAVzQcAALCKx6tfZN68eVo2adIk47p33XWXlu3cuTPiYwrHZZeZb2h6+umntWzGjBladv3112tZOLPAbWvrj6j+y1/+YsxDuYPrYqY7U0Ts3e1isnfvXi0bMGBAi8cTC9p67Ybi6quv1rL77rtPy+bPn2/cvqamJuJjcsr0So2TJ09q2XXXXWdjOBHB49UBAEDMofkAAABW0XwAAACraD4AAIBVPF79IqZHlJsyt/jyyy+Nuelxwg899JCWffe739UyN004beumTp1qzMeOHatldXV1WrZu3bqIj8nk5ptv1rJnnnnG8famSXtoOz766CMte+SRR6Iwkn9uwYIFWtatWzctW7ZsmY3hRBVXPgAAgFU0HwAAwKqQm48dO3bIyJEjJSMjQzwej2zYsKHJ55VSMnfuXOnRo4ckJSVJXl6eHDp0KFLjBVqM2oVbUbuINyE3H7W1tdK/f3/ja+JFzr/uevHixbJs2TLZvXu3dOzYUYYPH278ezJgE7ULt6J2EW9CnnA6YsQIGTFihPFzSil57rnnZPbs2TJq1CgREVm5cqWkpaXJhg0bZPz48eGNFhHz3nvvadmRI0e0rHfv3q0/GEvaYu3u3r07pDxaTJOdQ7Fv374IjSQ2tcXadbMxY8YYc6eTYE+cOBHJ4cSkiM75OHz4sFRWVkpeXl4w8/l8MnDgQCkpKYnkoYCIonbhVtQu3Ciit9pWVlaKiEhaWlqTPC0tLfi5i9XX10t9fX3w40AgEMkhAY5Qu3ArahduFPW7XQoKCsTn8wWXnj17RntIgCPULtyK2kW0RbT5SE9PFxGRqqqqJnlVVVXwcxebNWuW+P3+4FJeXh7JIQGOULtwK2oXbhTRP7tkZWVJenq6FBUVBV/FHggEZPfu3TJlyhTjNl6vV7xebySHAQeqq6u17PTp01rWVp4cSe1GV0ZGhpZ5PB7juh9++KGW/fd//3fEx+QW1G503X///Vr22GOPGddVSmnZiy++qGVLly4Nf2AxLuTm44svvmjyKNvDhw/Le++9J126dJHMzEyZPn26PPXUU3LNNddIVlaWzJkzRzIyMmT06NGRHDcQMmoXbkXtIt6E3Hzs2bOnyXsYZsyYISIiEyZMkBUrVsjMmTOltrZWJk+eLNXV1XLTTTfJm2++KR06dIjcqIEWoHbhVtQu4k3IzceQIUOMl44u8Hg8Mn/+fJk/f35YAwMijdqFW1G7iDdRv9sFAAC0LTQfAADAqoje7YL4853vfEfLfvnLX0ZhJIgXX30S5wVDhw7Vsub+zLBp0yYtO3bsWPgDAy7hySef1LKJEydqWY8ePYzbFxUVaVlzdyTFO658AAAAq2g+AACAVTQfAADAKpoPAABgFRNOoyQnJ8eYZ2dnO9r+rbfe0rIDBw6ENSaTq666KuL7RNv28MMPh7V9XV1dhEYCnPeDH/xAy2bNmqVlN954o5b9s+evXMz08930u+Dtt992vE+34soHAACwiuYDAABYRfMBAACsovkAAABWMeE0wq699lot+6//+i8tGzZsWFjHSUjQ+8ZFixY5OraIyPHjxx0dxzSxFYimdevWRXsIiDEdO3bUsjlz5mjZT37yE+P2Xbp00bLLLnP263HPnj1a9q1vfcu4bmpqqpa9/vrrWnbLLbc4Oo6bceUDAABYRfMBAACsovkAAABW0XwAAACrmHAahquvvlrL3nzzTS0zvV65uac07tq1S8u++93valljY6OWzZgxQ8tGjBhhPM60adOM+cX++te/OloPMElPT9cyp5Ot//SnPxnz/fv3hzUmuNuQIUO0bOHChVrm9GnRIuafx6dOndKyJ598UsuWLl3q+DgLFizQsoceekjLbrvtNi1jwikAAEAYaD4AAIBVNB8AAMAqmg8AAGAVE07DMGrUKC3LyMjQstmzZ2vZCy+8YNxnTU2Nlvl8Pi177LHHtOy+++7Tsr59+xqP88Ybb2iZ6ampQDi++c1vapnpFeQej8fGcBAHTE8PNU0uNdXZJ598YtznI488omUbN25swej+uUcffVTLTE9dnTlzppb9+c9/1rKdO3dGZmBRwG8bAABgFc0HAACwiuYDAABYRfMBAACsovkAAABWcbdLGCZOnKhlf//737XsN7/5jZZ99tlnjo9z4sQJLZs+fbqWde3aVcvuuusu4z7bt2/v+PhAS5lm7Tu1ZMmSCI4E8eLIkSNatmXLFi07fvy4lj344IPGfYby89iGDh06aFlWVpaWcbcLAACAQzQfAADAKpoPAABgFc0HAACwigmnEfbOO+9omWnCaGvIz8/XsurqauO6U6ZMcbTPBx54QMv++Mc/atk//vEPR/tD25KamupoPVP9mL6XgFdeecVRFot69+6tZabJpW0BVz4AAIBVNB8AAMAqmg8AAGAVzQcAALCKCadheOutt7TsRz/6kZZdffXVWvbRRx9FfDyBQEDLGhsbjet6PB5H++zXr5+W/exnP9OyJ5980tH+EL+Sk5O1LCUlRctMtfcv//IvWtbcRDy/39+C0QH29OrVy5ibfk6avkdef/11LXPLpFqnuPIBAACsovkAAABWhdR8FBQUyIABAyQ5OVm6d+8uo0ePlrKysibr1NXVSX5+vnTt2lU6deok48aNk6qqqogOGggVtQu3onYRj0JqPoqLiyU/P1927dolW7ZskbNnz8qwYcOktrY2uM6DDz4omzZtkrVr10pxcbFUVFTI2LFjIz5wIBTULtyK2kU88iilVEs3PnHihHTv3l2Ki4tl8ODB4vf75YorrpBVq1bJHXfcISIiH374ofTt21dKSkrkxhtvvOQ+A4GA+Hy+lg7JKtMkuaNHj2rZunXrtOzuu+827rO+vr7F47nnnnu0rLnXkpuefPr3v/9dy2644QYtM72q+gc/+IHxOKWlpcbcFr/fb5zQ1dZrtzX0799fy/bu3eto20OHDmlZdna2cd2v/tKNZ9Ru7GnXrp2W9e3bV8vmzp1r3H7cuHFadvr0aS2bOXOmli1dutTJEGNCc7X7VWHN+bgw67xLly4icv4XzdmzZyUvLy+4Tp8+fSQzM1NKSkrCORQQUdQu3IraRTxo8a22jY2NMn36dBk0aFDwdszKykpJTEzU3ueQlpYmlZWVxv3U19c3+d++6XZRIJKoXbgVtYt40eIrH/n5+XLgwAFZvXp1WAMoKCgQn88XXHr27BnW/oBLoXbhVtQu4kWLmo+pU6fK5s2bZdu2bXLllVcG8/T0dGloaNDmE1RVVUl6erpxX7NmzRK/3x9cysvLWzIkwBFqF25F7SKehPRnF6WUTJs2TdavXy/bt2+XrKysJp/Pzs6W9u3bS1FRUXBiTVlZmRw9elRyc3ON+/R6veL1els4/OgyvQZ8x44dWmaadd6xY0fjPk1PwPvkk0+0bPDgwVo2a9YsLWvucqppgujBgwe1zPRD6YorrtCy2267zXicaE84vYDajW2FhYVa1lYmll4KtRs7Xn75ZS0L5a6iU6dOadmYMWO07O233w5tYC4UUvORn58vq1atko0bN0pycnLw74k+n0+SkpLE5/PJpEmTZMaMGdKlSxdJSUmRadOmSW5urqMZ10BroXbhVtQu4lFIzceFW32GDBnSJF++fLlMnDhRRESeffZZSUhIkHHjxkl9fb0MHz5cnn/++YgMFmgpahduRe0iHoX8Z5dL6dChgxQWFhovowLRQu3CrahdxCPe7QIAAKyi+QAAAFa1+CFjMBs1apSW7dy5U8uGDx9u3L65/GIej0fLTI89v/XWW43bO70LpXfv3lo2Y8YMLXvllVcc7Q8wMT3uH3Dq61//upb16tVLy0yPNxcR+drXvuboON/73ve07K9//auW/epXvzJuv2vXLi07cOCAo2PHG658AAAAq2g+AACAVTQfAADAKpoPAABgFRNOI8z0OPPp06dr2ejRo43bDxs2TMuuuuoqLTM9xv3nP/+5loX7eHPT+Tz++ONh7RNth2liNOCEabK7iMhLL72kZVdffbWWde3a1fGxTK+weOedd7Tsnnvu0bINGzZo2eeff+742G0VVz4AAIBVNB8AAMAqmg8AAGAVzQcAALCKCacWFBUVOcqAeOPkpWiASefOnY35v/7rv2pZx44dtcxUe809ifmxxx7Tso8//vhSQ0QYuPIBAACsovkAAABW0XwAAACraD4AAIBVTDgFEBEnTpxwlF1xxRU2hgOXa+7pzCkpKZZHgtbAlQ8AAGAVzQcAALCK5gMAAFhF8wEAAKyi+QAAAFZxtwuAiKioqNCyX//611o2ceJELfvkk09aY0gAYhRXPgAAgFU0HwAAwCqaDwAAYBXNBwAAsMqjlFLRHsRXBQIB8fl80R4G4oTf77f2OGZqF5FE7cKtnNQuVz4AAIBVNB8AAMAqmg8AAGBVzDUfMTYFBS5ns56oXUQStQu3clJPMdd81NTURHsIiCM264naRSRRu3ArJ/UUc3e7NDY2SkVFhSQnJ0tNTY307NlTysvLrc36bk2BQIDzsUQpJTU1NZKRkSEJCXZ6bGrXPWL5fKjdyIrlf+uWiOXzCaV2Y+7dLgkJCXLllVeKiIjH4xERkZSUlJj7IoeD87HD9q2D1K77xOr5ULuRx/nY4bR2Y+7PLgAAIL7RfAAAAKtiuvnwer0yb9488Xq90R5KRHA+bUe8fW04n7Yj3r42nE9sirkJpwAAIL7F9JUPAAAQf2g+AACAVTQfAADAqphtPgoLC6VXr17SoUMHGThwoLz77rvRHpJjO3bskJEjR0pGRoZ4PB7ZsGFDk88rpWTu3LnSo0cPSUpKkry8PDl06FB0BnsJBQUFMmDAAElOTpbu3bvL6NGjpaysrMk6dXV1kp+fL127dpVOnTrJuHHjpKqqKkojjg1urV9ql9qldmNDvNdvTDYfa9askRkzZsi8efNk79690r9/fxk+fLgcP3482kNzpLa2Vvr37y+FhYXGzy9cuFAWL14sy5Ytk927d0vHjh1l+PDhUldXZ3mkl1ZcXCz5+fmya9cu2bJli5w9e1aGDRsmtbW1wXUefPBB2bRpk6xdu1aKi4uloqJCxo4dG8VRR5eb65fapXap3dgQ9/WrYlBOTo7Kz88Pfnzu3DmVkZGhCgoKojiqlhERtX79+uDHjY2NKj09XS1atCiYVVdXK6/Xq15++eUojDA0x48fVyKiiouLlVLnx96+fXu1du3a4DoffPCBEhFVUlISrWFGVbzUL7Xb9lC7sSve6jfmrnw0NDRIaWmp5OXlBbOEhATJy8uTkpKSKI4sMg4fPiyVlZVNzs/n88nAgQNdcX5+v19ERLp06SIiIqWlpXL27Nkm59OnTx/JzMx0xflEWjzXL7Ub36jd2BZv9RtzzcfJkyfl3LlzkpaW1iRPS0uTysrKKI0qci6cgxvPr7GxUaZPny6DBg2Sfv36icj580lMTJTU1NQm67rhfFpDPNcvtRvfqN3YFY/1G3MvlkPsys/PlwMHDsjbb78d7aEAIaF24WbxWL8xd+WjW7du0q5dO23GblVVlaSnp0dpVJFz4Rzcdn5Tp06VzZs3y7Zt24JvvxQ5fz4NDQ1SXV3dZP1YP5/WEs/1S+3GN2o3NsVr/cZc85GYmCjZ2dlSVFQUzBobG6WoqEhyc3OjOLLIyMrKkvT09CbnFwgEZPfu3TF5fkopmTp1qqxfv162bt0qWVlZTT6fnZ0t7du3b3I+ZWVlcvTo0Zg8n9YWz/VL7cY3aje2xH39RnnCq9Hq1auV1+tVK1asUAcPHlSTJ09WqampqrKyMtpDc6Smpkbt27dP7du3T4mIeuaZZ9S+ffvUp59+qpRSasGCBSo1NVVt3LhR7d+/X40aNUplZWWpM2fORHnkuilTpiifz6e2b9+ujh07FlxOnz4dXOf+++9XmZmZauvWrWrPnj0qNzdX5ebmRnHU0eXm+qV2qV1qNzbEe/3GZPOhlFJLlixRmZmZKjExUeXk5Khdu3ZFe0iObdu2TYmItkyYMEEpdf62rzlz5qi0tDTl9XrV0KFDVVlZWXQH3QzTeYiIWr58eXCdM2fOqAceeEB17txZXX755WrMmDHq2LFj0Rt0DHBr/VK71C61GxvivX55qy0AALAq5uZ8AACA+EbzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAqmg8AAGAVzQcAALCK5gMAAFhF8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYdVlr7biwsFAWLVoklZWV0r9/f1myZInk5ORccrvGxkapqKiQ5ORk8Xg8rTU8xDmllNTU1EhGRoYkJITWY1O7iCZqF24VUu2qVrB69WqVmJiofve736n3339f3XvvvSo1NVVVVVVdctvy8nIlIiwsEVnKy8upXRZXLtQui1sXJ7XbKs1HTk6Oys/PD3587tw5lZGRoQoKCi65bXV1ddS/cCzxs1RXV1O7LK5cqF0Wty5Oajficz4aGhqktLRU8vLygllCQoLk5eVJSUmJtn59fb0EAoHgUlNTE+khoQ0L5RIytYtYQu3CrZzUbsSbj5MnT8q5c+ckLS2tSZ6WliaVlZXa+gUFBeLz+YJLz549Iz0kwBFqF25F7cJton63y6xZs8Tv9weX8vLyaA8JcITahVtRu4i2iN/t0q1bN2nXrp1UVVU1yauqqiQ9PV1b3+v1itfrjfQwgJBRu3ArahduE/ErH4mJiZKdnS1FRUXBrLGxUYqKiiQ3NzfShwMihtqFW1G7cJ2QplM7tHr1auX1etWKFSvUwYMH1eTJk1VqaqqqrKy85LZ+vz/qM3VZ4mfx+/3ULosrF2qXxa2Lk9ptleZDKaWWLFmiMjMzVWJiosrJyVG7du1ytB3fBCyRXEL9AU7tssTKQu2yuHVxUrsepZSSGBIIBMTn80V7GIgTfr9fUlJSrByL2kUkUbtwKye1G/W7XQAAQNtC8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFU0HwAAwKqIP14dAACbHn74YS1LSkrSsoceesi4vendNp999pmWffUJsi2xatUqLfv444/D2qdbceUDAABYRfMBAACsovkAAABW0XwAAACreLcL4lo8vB/D6/Vq2ZgxY7TsuuuuC+s43/72t7XsnXfeCWufBQUFWlZfX69lDQ0NYR0nHsVD7baGU6dOaVlqaqr9gfwfj8ejZc39Wj137pyWmb6XN2/eHP7Aooh3uwAAgJhD8wEAAKyi+QAAAFbRfAAAAKt4wikQQ0yTS3fv3q1l4U4uNamrq9Oyr3/9646379Spk5bNmjVLy4qLi7XMNDF1x44dxuOcOXPG8ZgQf/7yl79o2fe+9z1H2y5fvtyYBwKBFo/nhz/8oZb16NHDuG67du20bMmSJVr24YcfatlHH33UgtHFLq58AAAAq2g+AACAVTQfAADAKpoPAABgFRNOXap3795aNmXKFC0L9wG2polTQ4cO1bLXXnvNuP1vfvMbLTNNoMR5jY2NWva3v/1Ny0wTTk1PDjW9KlxE5KWXXtKyt956S8u2b99u3N7ENCbTK8xvuukmLXvjjTe0bOXKlcbjTJw40fGYEH9Gjx6tZbfffruW/fnPf9ay6upq4z5NTx516vHHH9cy09OCRcw/J7/2ta9p2Te/+U0tY8IpAABAGGg+AACAVTQfAADAKpoPAABglUeFOyMxwtz0audwvPrqq8Y8KyvL0fam9UyvMA73nzeU10Wb7Ny5U8sGDx4c1phCEQ+vJU9KStKy2267TctMk+lMk+6izXQ+pol4119/vXH7X/3qV1o2d+7csMcVa+Khdtsq06Rqkeaf2nuxPXv2aFlOTk5YY7LJSe1y5QMAAFhF8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFU8Xj1KTI8IFjHfSfL+++9rWU1NjZaZZheXlpYaj7Nu3Tot27t3r5aNGzdOy+69917jPk22bt3qeF2YnTlzRsteeeWVKIwkMkznc+TIES0bMmSIcfv8/HwtW7NmjZaZvm8AN0hOTo72EFodVz4AAIBVNB8AAMAqmg8AAGAVzQcAALAqLiec9u7dW8s++eSTKIykeQkJ5r7vzjvv1DLTI7I7d+6sZeGeo+nYkydPdrTtvn37jPnjjz8ezpDgct/4xje07LrrrtOyW2+91fE+TbVvygDELq58AAAAq2g+AACAVTQfAADAqpCbjx07dsjIkSMlIyNDPB6PbNiwocnnlVIyd+5c6dGjhyQlJUleXp4cOnQoUuMFWozahVtRu4g3IU84ra2tlf79+8vPfvYzGTt2rPb5hQsXyuLFi+XFF1+UrKwsmTNnjgwfPlwOHjwoHTp0iMigLyXWJpeGYu3atY7W+/zzz7XM9FS8O+64w7i96d8uLy/P0bE3bdqkZT/+8Y8dbRtNbqjdeLNjxw4tu+KKK6IwEnejdmOXqZ7nzZsX1j4LCwvD2t4NQm4+RowYISNGjDB+Tiklzz33nMyePVtGjRolIiIrV66UtLQ02bBhg4wfPz680QJhoHbhVtQu4k1E53wcPnxYKisrm/wP2ufzycCBA6WkpMS4TX19vQQCgSYLYBu1C7eiduFGEW0+KisrRUQkLS2tSZ6Wlhb83MUKCgrE5/MFl549e0ZySIAj1C7citqFG0X9bpdZs2aJ3+8PLuXl5dEeEuAItQu3onYRbRF9wml6erqIiFRVVUmPHj2CeVVVlVx//fXGbbxer3i93kgOI+507dpVy77//e9r2fz587WsV69ejo9TWlqqZS+//LKWLVmyRMu+/PJLx8eJRdRu6zBNvHv++efD2uekSZO0zFS7bQW1a49pcumqVau0bOjQoY73+dprr2nZ//zP/4Q2MBeK6JWPrKwsSU9Pl6KiomAWCARk9+7dkpubG8lDARFF7cKtqF24UchXPr744gv56KOPgh8fPnxY3nvvPenSpYtkZmbK9OnT5amnnpJrrrkmeMtXRkaGjB49OpLjBkJG7cKtqF3Em5Cbjz179sjNN98c/HjGjBkiIjJhwgRZsWKFzJw5U2pra2Xy5MlSXV0tN910k7z55pvca46oo3bhVtQu4k3IzceQIUNEKdXs5z0ej8yfP984/wCIJmoXbkXtIt5E/W4XAADQtkT0bheIJCUlaZnpEeff/va3jdtnZWVp2Y033qhlpkepezweLftn/1u62OHDh7Xs2Wefdbw9cLEXXnjB0Xqh3AEzffp0LfvTn/6kZWfOnHG8T+BirXFny//+7/9qWUFBgZaZXp8Rb7jyAQAArKL5AAAAVtF8AAAAq2g+AACAVR4VyoxECwKBgPh8vmgPownT481FzJPcbrjhhtYejlXHjh3TMtMEqw8//NDGcELm9/slJSXFyrFisXZjUUKC/n+eO++8U8tMj2YXEenTp4+Wvf/++1r21be8XlBVVeVkiDGB2rUnOztbyxYsWKBlTieXHjhwwJgPGTJEy06dOuVon27ipHa58gEAAKyi+QAAAFbRfAAAAKtoPgAAgFU84dSBa665xph37NhRy5zO3zU9jVREpLa2Vsuee+45LVu3bp2WmSaH3nLLLcbjLFy4UMsyMjK0rEePHlqWnp6uZbE64RSxp7GxUcvWrFmjZcXFxcbtt2zZomX9+vXTsu3bt2uZ6fvB9H2D+NS/f39jbvp5mpmZ6Wifpp99Tz/9tHHdeJxc2lJc+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqecBqGpKQkLWtucqpTdXV1Wva3v/0trH2azJ49W8ueeOIJLWtoaNAy00SsEydORGZgEcZTIuOPaWL0jh07tKx3795a9sEHH2hZbm6u8TiBQKAFo4scajc8d999t5YtW7bMuK7pZ7np3//RRx/Vsj/84Q9a1tYnlvKEUwAAEHNoPgAAgFU0HwAAwCqaDwAAYBVPOA3DmTNntGz//v1RGEnzOnfubMzvueceR9ubnjIZq5NL0TZUVFRo2a9//WstKygo0LK+fftqmWkCq0j0J5zCud/97ndaNn78eC3r0KGDcfuamhot++lPf6pl69evb8HoYMKVDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVnG3S5xbvHixMTc9It3k1VdfjeRwgFYRzl1mU6ZMMeb/8R//0eJ9ovXcddddWvZv//ZvWpaYmKhltbW1xn1OmDBBy/bs2aNlvXr1cjBC82spRMx3arVVXPkAAABW0XwAAACraD4AAIBVNB8AAMAqJpy6VO/evbXs97//vZYNGDDAuL1SSsteeOEFRxkQa3r27NnibW+//XZj/vOf/1zL6urqWnwchGbEiBHG/MUXX9Syyy5z9qussrLSmI8cOVLLfvvb32pZc6+ruNjx48eN+WuvvaZlVVVVWlZYWOjoONXV1cbcVKc//vGPtcx0Q4HpUfOtgSsfAADAKpoPAABgFc0HAACwiuYDAABYxYRTF7jyyiu1zDRx6Rvf+IaWNfekveeff17LZs+e3YLRAXZ961vf0rJp06ZFYSRoTQMHDjTmTieXmlx11VUh5S3VvXt3Y/7Tn/7U0faPPvqoo/W2b99uzD/99FMtMz3FddKkSVo2ePBgR8cOF1c+AACAVTQfAADAKpoPAABgVUjNR0FBgQwYMECSk5Ole/fuMnr0aCkrK2uyTl1dneTn50vXrl2lU6dOMm7cOONDVACbqF24FbWLeORRpkddNuP73/++jB8/XgYMGCBffvml/OIXv5ADBw7IwYMHpWPHjiJy/vXUr732mqxYsUJ8Pp9MnTpVEhISZOfOnY6OEQgExOfztexs/o/pyYQff/yxlr3yyithHSccd999tzEfN26clg0ZMkTLkpOTteydd97Rsjlz5hiP09xEpXjj9/slJSXFNbVr0qlTJy176aWXtGzfvn1a1twTapt70mO0mCaR5ubmGtd96qmntMz0/WBy5MgRLWvuSZoX/4K3LR5q1ymv16tlzT1pM5wJp7W1tcb82Wef1bLPPvusxcdp7om79913n5Zd+DeMBtPXOBJ1cKF2/5mQ/hXffPPNJh+vWLFCunfvLqWlpTJ48GDx+/3y29/+VlatWiW33HKLiIgsX75c+vbtK7t27ZIbb7wxxFMAIoPahVtRu4hHYc358Pv9IiLSpUsXEREpLS2Vs2fPSl5eXnCdPn36SGZmppSUlBj3UV9fL4FAoMkCtDZqF25F7SIetLj5aGxslOnTp8ugQYOkX79+InL+Um5iYqKkpqY2WTctLa3Zy7wFBQXi8/mCSzgviAKcoHbhVtQu4kWLm4/8/Hw5cOCArF69OqwBzJo1S/x+f3ApLy8Pa3/ApVC7cCtqF/GiRTN3pk6dKps3b5YdO3Y0efpmenq6NDQ0SHV1dZMuvKqqStLT04378nq9xslG4ViwYIGWnTp1Sssu/M/hqw4fPuz4OBf+vvpVSUlJWnbHHXc43qeJaZLc008/rWULFy4M6zhtQazXrskXX3yhZaaJazNmzNCy0tJS4z5Pnz7t6Nj79+/Xss2bNzvaVkTk5ptv1jLTRFLTEyGdTiJtzhtvvKFlpidMNvf681jjxtp1aubMmVoWysTSxsZGLdu9e7eW3XrrrcbtL/wpq7X98pe/1DLTq+7Hjx+vZddee62WbdmyxXicQYMGaVlBQYGWFRcXG7e3IaQrH0opmTp1qqxfv162bt0qWVlZTT6fnZ0t7du3l6KiomBWVlYmR48ebXbmOmADtQu3onYRj0K68pGfny+rVq2SjRs3SnJycvDviT6fT5KSksTn88mkSZNkxowZ0qVLF0lJSZFp06ZJbm4uM64RVdQu3IraRTwKqflYunSpiOjPnVi+fLlMnDhRRM7fL52QkCDjxo2T+vp6GT58uPElZoBN1C7citpFPAqp+XDyPLIOHTpIYWGhFBYWtnhQQKRRu3ArahfxiHe7AAAAq0J6vLoNkXjM77Jly7Ts3nvvbfH+PB6PMQ/nS2e6i0DE/Djs3//+91rW3KOH0ZSTx/xGSrQfUf3VOyAueOSRRxxvf9ddd2lZt27dwhqT6Xsn3B85//jHP7Rs0aJFWrZy5Uotq66uDuvYNrWl2jW9BuKJJ55wvP2dd96pZevWrQtrTGg5J7XLlQ8AAGAVzQcAALCK5gMAAFhF8wEAAKyKywmnpscyX/xUwEi44YYbHK23d+9eLWtuwikiqy1N2guX6dUACQn6/0/+/d//3bh9p06dtOzLL7/UMtPzJ0zvKmlusuz777+vZWfOnDGu62bULtyKCacAACDm0HwAAACraD4AAIBVNB8AAMCquJxwClzApD24FbULt2LCKQAAiDk0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAqmg8AAGBVzDUfSqloDwFxxGY9UbuIJGoXbuWknmKu+aipqYn2EBBHbNYTtYtIonbhVk7qyaNirOVtbGyUiooKSU5OlpqaGunZs6eUl5dLSkpKtIcWtkAgwPlYopSSmpoaycjIkIQEOz02tesesXw+1G5kxfK/dUvE8vmEUruXWRqTYwkJCXLllVeKiIjH4xERkZSUlJj7IoeD87HD5/NZPR616z6xej7UbuRxPnY4rd2Y+7MLAACIbzQfAADAqphuPrxer8ybN0+8Xm+0hxIRnE/bEW9fG86n7Yi3rw3nE5tibsIpAACIbzF95QMAAMQfmg8AAGAVzQcAALCK5gMAAFgVs81HYWGh9OrVSzp06CADBw6Ud999N9pDcmzHjh0ycuRIycjIEI/HIxs2bGjyeaWUzJ07V3r06CFJSUmSl5cnhw4dis5gL6GgoEAGDBggycnJ0r17dxk9erSUlZU1Waeurk7y8/Ola9eu0qlTJxk3bpxUVVVFacSxwa31S+1Su9RubIj3+o3J5mPNmjUyY8YMmTdvnuzdu1f69+8vw4cPl+PHj0d7aI7U1tZK//79pbCw0Pj5hQsXyuLFi2XZsmWye/du6dixowwfPlzq6uosj/TSiouLJT8/X3bt2iVbtmyRs2fPyrBhw6S2tja4zoMPPiibNm2StWvXSnFxsVRUVMjYsWOjOOrocnP9UrvULrUbG+K+flUMysnJUfn5+cGPz507pzIyMlRBQUEUR9UyIqLWr18f/LixsVGlp6erRYsWBbPq6mrl9XrVyy+/HIURhub48eNKRFRxcbFS6vzY27dvr9auXRtc54MPPlAiokpKSqI1zKiKl/qldtseajd2xVv9xtyVj4aGBiktLZW8vLxglpCQIHl5eVJSUhLFkUXG4cOHpbKyssn5+Xw+GThwoCvOz+/3i4hIly5dRESktLRUzp492+R8+vTpI5mZma44n0iL5/qlduMbtRvb4q1+Y675OHnypJw7d07S0tKa5GlpaVJZWRmlUUXOhXNw4/k1NjbK9OnTZdCgQdKvXz8ROX8+iYmJkpqa2mRdN5xPa4jn+qV24xu1G7visX5j7q22iF35+fly4MABefvtt6M9FCAk1C7cLB7rN+aufHTr1k3atWunzditqqqS9PT0KI0qci6cg9vOb+rUqbJ582bZtm1b8NXbIufPp6GhQaqrq5usH+vn01riuX6p3fhG7cameK3fmGs+EhMTJTs7W4qKioJZY2OjFBUVSW5ubhRHFhlZWVmSnp7e5PwCgYDs3r07Js9PKSVTp06V9evXy9atWyUrK6vJ57Ozs6V9+/ZNzqesrEyOHj0ak+fT2uK5fqnd+Ebtxpa4r98oT3g1Wr16tfJ6vWrFihXq4MGDavLkySo1NVVVVlZGe2iO1NTUqH379ql9+/YpEVHPPPOM2rdvn/r000+VUkotWLBApaamqo0bN6r9+/erUaNGqaysLHXmzJkoj1w3ZcoU5fP51Pbt29WxY8eCy+nTp4Pr3H///SozM1Nt3bpV7dmzR+Xm5qrc3Nwojjq63Fy/1C61S+3Ghniv35hsPpRSasmSJSozM1MlJiaqnJwctWvXrmgPybFt27YpEdGWCRMmKKXO3/Y1Z84clZaWprxerxo6dKgqKyuL7qCbYToPEVHLly8PrnPmzBn1wAMPqM6dO6vLL79cjRkzRh07dix6g44Bbq1fapfapXZjQ7zXr0cppVr32goAAMD/F3NzPgAAQHyj+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVf8PQ3vxzS771oYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example iterator\n",
    "itr = iter(train_loader)\n",
    "example_X, example_Y = next(itr) # it return next batch of data from dataset\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_X[i][0], cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/24], Step [100/600], Loss: 0.4002\n",
      "Epoch [1/24], Step [200/600], Loss: 0.1712\n",
      "Epoch [1/24], Step [300/600], Loss: 0.0999\n",
      "Epoch [1/24], Step [400/600], Loss: 0.2968\n",
      "Epoch [1/24], Step [500/600], Loss: 0.1256\n",
      "Epoch [1/24], Step [600/600], Loss: 0.1022\n",
      "Epoch [2/24], Step [100/600], Loss: 0.0840\n",
      "Epoch [2/24], Step [200/600], Loss: 0.1044\n",
      "Epoch [2/24], Step [300/600], Loss: 0.0950\n",
      "Epoch [2/24], Step [400/600], Loss: 0.2278\n",
      "Epoch [2/24], Step [500/600], Loss: 0.1666\n",
      "Epoch [2/24], Step [600/600], Loss: 0.1369\n",
      "Epoch [3/24], Step [100/600], Loss: 0.1383\n",
      "Epoch [3/24], Step [200/600], Loss: 0.0629\n",
      "Epoch [3/24], Step [300/600], Loss: 0.0231\n",
      "Epoch [3/24], Step [400/600], Loss: 0.0367\n",
      "Epoch [3/24], Step [500/600], Loss: 0.1058\n",
      "Epoch [3/24], Step [600/600], Loss: 0.0943\n",
      "Epoch [4/24], Step [100/600], Loss: 0.0160\n",
      "Epoch [4/24], Step [200/600], Loss: 0.0943\n",
      "Epoch [4/24], Step [300/600], Loss: 0.0116\n",
      "Epoch [4/24], Step [400/600], Loss: 0.0145\n",
      "Epoch [4/24], Step [500/600], Loss: 0.1012\n",
      "Epoch [4/24], Step [600/600], Loss: 0.0283\n",
      "Epoch [5/24], Step [100/600], Loss: 0.0401\n",
      "Epoch [5/24], Step [200/600], Loss: 0.0304\n",
      "Epoch [5/24], Step [300/600], Loss: 0.0641\n",
      "Epoch [5/24], Step [400/600], Loss: 0.0162\n",
      "Epoch [5/24], Step [500/600], Loss: 0.0822\n",
      "Epoch [5/24], Step [600/600], Loss: 0.0516\n",
      "Epoch [6/24], Step [100/600], Loss: 0.0133\n",
      "Epoch [6/24], Step [200/600], Loss: 0.0070\n",
      "Epoch [6/24], Step [300/600], Loss: 0.0188\n",
      "Epoch [6/24], Step [400/600], Loss: 0.0194\n",
      "Epoch [6/24], Step [500/600], Loss: 0.0369\n",
      "Epoch [6/24], Step [600/600], Loss: 0.0148\n",
      "Epoch [7/24], Step [100/600], Loss: 0.0181\n",
      "Epoch [7/24], Step [200/600], Loss: 0.0869\n",
      "Epoch [7/24], Step [300/600], Loss: 0.0287\n",
      "Epoch [7/24], Step [400/600], Loss: 0.0142\n",
      "Epoch [7/24], Step [500/600], Loss: 0.0125\n",
      "Epoch [7/24], Step [600/600], Loss: 0.0605\n",
      "Epoch [8/24], Step [100/600], Loss: 0.0042\n",
      "Epoch [8/24], Step [200/600], Loss: 0.0407\n",
      "Epoch [8/24], Step [300/600], Loss: 0.0054\n",
      "Epoch [8/24], Step [400/600], Loss: 0.0152\n",
      "Epoch [8/24], Step [500/600], Loss: 0.0213\n",
      "Epoch [8/24], Step [600/600], Loss: 0.0217\n",
      "Epoch [9/24], Step [100/600], Loss: 0.0120\n",
      "Epoch [9/24], Step [200/600], Loss: 0.0193\n",
      "Epoch [9/24], Step [300/600], Loss: 0.0364\n",
      "Epoch [9/24], Step [400/600], Loss: 0.0033\n",
      "Epoch [9/24], Step [500/600], Loss: 0.0276\n",
      "Epoch [9/24], Step [600/600], Loss: 0.0136\n",
      "Epoch [10/24], Step [100/600], Loss: 0.0225\n",
      "Epoch [10/24], Step [200/600], Loss: 0.0182\n",
      "Epoch [10/24], Step [300/600], Loss: 0.0261\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected neural network\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l2(self.relu(self.l1(x)))\n",
    "        # no activation or softmax as Op func \n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    # each iteration will give a batch of images \n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        # original shape : [100, 1, 28, 28]\n",
    "        # now: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward prop\n",
    "        outputs = model.forward(images)\n",
    "        loss = loss_f(outputs, labels)\n",
    "\n",
    "        # backward prop and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    good_pred = 0\n",
    "    total_samples = len(test_loader.dataset)\n",
    "    \n",
    "    for images,labels in (test_loader):\n",
    "        # 1oo images per batch\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model.forward(images)\n",
    "        _, prediction = torch.max(outputs, 1) # max give (output value, index)\n",
    "\n",
    "        good_pred += (prediction==labels).sum().item()\n",
    "\n",
    "    accuracy = good_pred/(total_samples)\n",
    "    print(f'Accuracy of the network on the {total_samples} test images: {100 * accuracy} %')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
