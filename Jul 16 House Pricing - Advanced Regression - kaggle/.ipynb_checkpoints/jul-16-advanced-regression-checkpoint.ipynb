{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as skl\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# device11\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "1455          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0            Lvl    AllPub    Inside  ...        0    NaN    NaN         NaN   \n",
       "1            Lvl    AllPub       FR2  ...        0    NaN    NaN         NaN   \n",
       "2            Lvl    AllPub    Inside  ...        0    NaN    NaN         NaN   \n",
       "3            Lvl    AllPub    Corner  ...        0    NaN    NaN         NaN   \n",
       "4            Lvl    AllPub       FR2  ...        0    NaN    NaN         NaN   \n",
       "...          ...       ...       ...  ...      ...    ...    ...         ...   \n",
       "1455         Lvl    AllPub    Inside  ...        0    NaN    NaN         NaN   \n",
       "1456         Lvl    AllPub    Inside  ...        0    NaN  MnPrv         NaN   \n",
       "1457         Lvl    AllPub    Inside  ...        0    NaN  GdPrv        Shed   \n",
       "1458         Lvl    AllPub    Inside  ...        0    NaN    NaN         NaN   \n",
       "1459         Lvl    AllPub    Inside  ...        0    NaN    NaN         NaN   \n",
       "\n",
       "     MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0          0      2    2008        WD         Normal     208500  \n",
       "1          0      5    2007        WD         Normal     181500  \n",
       "2          0      9    2008        WD         Normal     223500  \n",
       "3          0      2    2006        WD        Abnorml     140000  \n",
       "4          0     12    2008        WD         Normal     250000  \n",
       "...      ...    ...     ...       ...            ...        ...  \n",
       "1455       0      8    2007        WD         Normal     175000  \n",
       "1456       0      2    2010        WD         Normal     210000  \n",
       "1457    2500      5    2010        WD         Normal     266500  \n",
       "1458       0      4    2010        WD         Normal     142125  \n",
       "1459       0      6    2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 80 columns]"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./train.csv\")\n",
    "df_test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_train = df_train.drop(columns=['Id'])\n",
    "\n",
    "# df_test = df_test.reset_index(drop=True)\n",
    "# df_test = df_test.drop(columns=['Id'])\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "        'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "        'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "        'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "        'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "        'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "        'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "        'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "        'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "        'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "        'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "        'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "        'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "        'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "        'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "        'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "        'SaleCondition'],\n",
       "       dtype='object'),\n",
       " (80,))"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns, df_test.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "cols_dropped = []\n",
    "\n",
    "# train\n",
    "for col in df_train.columns:\n",
    "    nan_percentage = (df_train[col].isna()==True).sum() * 100 / df_train.shape[0]\n",
    "    if nan_percentage > 60.0:\n",
    "        cols_dropped.insert(len(cols_dropped),col)\n",
    "        df_train = df_train.drop(col, axis=1)\n",
    "\n",
    "# test\n",
    "for col in cols_dropped:\n",
    "    df_test = df_test.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0             60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1             20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2             60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3             70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4             60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "...          ...      ...          ...      ...    ...      ...         ...   \n",
       "1455          60       RL         62.0     7917   Pave      Reg         Lvl   \n",
       "1456          20       RL         85.0    13175   Pave      Reg         Lvl   \n",
       "1457          70       RL         66.0     9042   Pave      Reg         Lvl   \n",
       "1458          20       RL         68.0     9717   Pave      Reg         Lvl   \n",
       "1459          20       RL         75.0     9937   Pave      Reg         Lvl   \n",
       "\n",
       "     Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0       AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1       AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2       AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3       AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4       AllPub       FR2       Gtl  ...             0         0           0   \n",
       "...        ...       ...       ...  ...           ...       ...         ...   \n",
       "1455    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1456    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1457    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1458    AllPub    Inside       Gtl  ...           112         0           0   \n",
       "1459    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "\n",
       "     PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0           0       0       2    2008        WD         Normal    208500  \n",
       "1           0       0       5    2007        WD         Normal    181500  \n",
       "2           0       0       9    2008        WD         Normal    223500  \n",
       "3           0       0       2    2006        WD        Abnorml    140000  \n",
       "4           0       0      12    2008        WD         Normal    250000  \n",
       "...       ...     ...     ...     ...       ...            ...       ...  \n",
       "1455        0       0       8    2007        WD         Normal    175000  \n",
       "1456        0       0       2    2010        WD         Normal    210000  \n",
       "1457        0    2500       5    2010        WD         Normal    266500  \n",
       "1458        0       0       4    2010        WD         Normal    142125  \n",
       "1459        0       0       6    2008        WD         Normal    147500  \n",
       "\n",
       "[1460 rows x 76 columns]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# handling categorical value\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# TRAIN\n",
    "le = LabelEncoder()\n",
    "for col in df_train.columns:\n",
    "#     print(df_train[col].dtype)\n",
    "    if(df_train[col].dtype == \"object\"):\n",
    "        df_train[col]=df_train[col].astype(str)\n",
    "        df_train[col] = le.fit_transform(df_train[col])\n",
    "    elif(df_train[col].dtype == \"int\"):\n",
    "        df_train[col]=df_train[col].astype(float)\n",
    "        df_train[col] = le.fit_transform(df_train[col])\n",
    "    else:\n",
    "        df_train[col].fillna(0,inplace=True)\n",
    "        \n",
    "\n",
    "df_train=df_train.astype(float)\n",
    "print(df_train.isnull().sum().sum())\n",
    "\n",
    "df_train = torch.tensor(np.array(df_train), dtype=torch.float32)\n",
    "\n",
    "# TEST\n",
    "for col in df_test.columns:\n",
    "    if(col==\"Id\"): continue\n",
    "#     print(df_test[col].dtype)\n",
    "    if(df_test[col].dtype == \"object\"):\n",
    "        df_test[col]=df_test[col].astype(str)\n",
    "        df_test[col] = le.fit_transform(df_test[col])\n",
    "    elif(df_test[col].dtype == \"int\"):\n",
    "        df_test[col]=df_test[col].astype(float)\n",
    "        df_test[col] = le.fit_transform(df_test[col])\n",
    "    else:\n",
    "        df_test[col].fillna(0,inplace=True)\n",
    "\n",
    "df_test=df_test.astype(float)\n",
    "print(df_test.isnull().sum().sum())\n",
    "df_test.to_csv(\"new_df_test.csv\")\n",
    "\n",
    "df_test = torch.tensor(np.array(df_test), dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,   3.,  65.,  ...,   8.,   4., 412.],\n",
       "        [  0.,   3.,  80.,  ...,   8.,   4., 339.],\n",
       "        [  5.,   3.,  68.,  ...,   8.,   4., 442.],\n",
       "        ...,\n",
       "        [  6.,   3.,  66.,  ...,   8.,   4., 527.],\n",
       "        [  0.,   3.,  68.,  ...,   8.,   4., 199.],\n",
       "        [  0.,   3.,  75.,  ...,   8.,   4., 221.]])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Adding the first hidden layer and its activation\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Adding the subsequent hidden layers and their activations\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Adding the output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Combining all layers into a sequential container\n",
    "        self.all_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        return self.all_layers(x)\n",
    "\n",
    "    def compute_l2_loss(self, w):\n",
    "          return torch.square(w).sum()\n",
    "\n",
    "def one_hot_vec(index, cols, rows):\n",
    "    res = torch.zeros(rows, cols)\n",
    "    for i in range(index.shape[0]):\n",
    "        res[i,index[i]]=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 253.01240158081055\n",
      "Epoch [2/300], Loss: 181.3558111190796\n",
      "Epoch [3/300], Loss: 128.83408737182617\n",
      "Epoch [4/300], Loss: 91.44210910797119\n",
      "Epoch [5/300], Loss: 65.17262101173401\n",
      "Epoch [6/300], Loss: 46.776068449020386\n",
      "Epoch [7/300], Loss: 33.87040710449219\n",
      "Epoch [8/300], Loss: 24.77304255962372\n",
      "Epoch [9/300], Loss: 18.311702966690063\n",
      "Epoch [10/300], Loss: 13.676701664924622\n",
      "Epoch [11/300], Loss: 10.313053369522095\n",
      "Epoch [12/300], Loss: 7.842203766107559\n",
      "Epoch [13/300], Loss: 6.0059371292591095\n",
      "Epoch [14/300], Loss: 4.627047806978226\n",
      "Epoch [15/300], Loss: 3.5824755281209946\n",
      "Epoch [16/300], Loss: 2.7854077368974686\n",
      "Epoch [17/300], Loss: 2.1735508143901825\n",
      "Epoch [18/300], Loss: 1.7014899179339409\n",
      "Epoch [19/300], Loss: 1.3356734737753868\n",
      "Epoch [20/300], Loss: 1.0510607212781906\n",
      "Epoch [21/300], Loss: 0.8288194462656975\n",
      "Epoch [22/300], Loss: 0.6547030434012413\n",
      "Epoch [23/300], Loss: 0.5178805161267519\n",
      "Epoch [24/300], Loss: 0.4100793059915304\n",
      "Epoch [25/300], Loss: 0.3249513618648052\n",
      "Epoch [26/300], Loss: 0.25760212168097496\n",
      "Epoch [27/300], Loss: 0.20423951745033264\n",
      "Epoch [28/300], Loss: 0.16191172506660223\n",
      "Epoch [29/300], Loss: 0.1283108675852418\n",
      "Epoch [30/300], Loss: 0.1016252376139164\n",
      "Epoch [31/300], Loss: 0.08042771834880114\n",
      "Epoch [32/300], Loss: 0.06359074637293816\n",
      "Epoch [33/300], Loss: 0.0502212995197624\n",
      "Epoch [34/300], Loss: 0.039610694628208876\n",
      "Epoch [35/300], Loss: 0.03119567164685577\n",
      "Epoch [36/300], Loss: 0.024528014939278364\n",
      "Epoch [37/300], Loss: 0.019250722252763808\n",
      "Epoch [38/300], Loss: 0.015079283039085567\n",
      "Epoch [39/300], Loss: 0.011786849761847407\n",
      "Epoch [40/300], Loss: 0.009192523662932217\n",
      "Epoch [41/300], Loss: 0.007152053789468482\n",
      "Epoch [42/300], Loss: 0.005550442234380171\n",
      "Epoch [43/300], Loss: 0.004296057304600254\n",
      "Epoch [44/300], Loss: 0.0033159351150970906\n",
      "Epoch [45/300], Loss: 0.002552032528910786\n",
      "Epoch [46/300], Loss: 0.0019582352761062793\n",
      "Epoch [47/300], Loss: 0.0014979567640693858\n",
      "Epoch [48/300], Loss: 0.0011422195748309605\n",
      "Epoch [49/300], Loss: 0.0008681185681780335\n",
      "Epoch [50/300], Loss: 0.0006575876213901211\n",
      "Epoch [51/300], Loss: 0.0004964115742041031\n",
      "Epoch [52/300], Loss: 0.00037343493750086054\n",
      "Epoch [53/300], Loss: 0.00027992765353701543\n",
      "Epoch [54/300], Loss: 0.00020907858652208233\n",
      "Epoch [55/300], Loss: 0.00015559056191705167\n",
      "Epoch [56/300], Loss: 0.00011535739031387493\n",
      "Epoch [57/300], Loss: 8.520713709003758e-05\n",
      "Epoch [58/300], Loss: 6.26983210167964e-05\n",
      "Epoch [59/300], Loss: 4.595866357703926e-05\n",
      "Epoch [60/300], Loss: 3.3557806546014035e-05\n",
      "Epoch [61/300], Loss: 2.440720083995984e-05\n",
      "Epoch [62/300], Loss: 1.768173933669459e-05\n",
      "Epoch [63/300], Loss: 1.2758491266140481e-05\n",
      "Epoch [64/300], Loss: 9.169087945792853e-06\n",
      "Epoch [65/300], Loss: 6.562822250089084e-06\n",
      "Epoch [66/300], Loss: 4.678195125507045e-06\n",
      "Epoch [67/300], Loss: 3.321043578807803e-06\n",
      "Epoch [68/300], Loss: 2.3478185369185667e-06\n",
      "Epoch [69/300], Loss: 1.6528504076518402e-06\n",
      "Epoch [70/300], Loss: 1.1586886330405832e-06\n",
      "Epoch [71/300], Loss: 8.088144198836744e-07\n",
      "Epoch [72/300], Loss: 5.621658409893371e-07\n",
      "Epoch [73/300], Loss: 3.890429471908874e-07\n",
      "Epoch [74/300], Loss: 2.680599919813176e-07\n",
      "Epoch [75/300], Loss: 1.838870833381634e-07\n",
      "Epoch [76/300], Loss: 1.2558507522797413e-07\n",
      "Epoch [77/300], Loss: 8.538370366295567e-08\n",
      "Epoch [78/300], Loss: 5.7788838603300974e-08\n",
      "Epoch [79/300], Loss: 3.893375910557495e-08\n",
      "Epoch [80/300], Loss: 2.6109777562233205e-08\n",
      "Epoch [81/300], Loss: 1.7428290810528324e-08\n",
      "Epoch [82/300], Loss: 1.1578728531524973e-08\n",
      "Epoch [83/300], Loss: 7.655982459109367e-09\n",
      "Epoch [84/300], Loss: 5.037948669350811e-09\n",
      "Epoch [85/300], Loss: 3.2991137183246977e-09\n",
      "Epoch [86/300], Loss: 2.149854516630967e-09\n",
      "Epoch [87/300], Loss: 1.3940114063082376e-09\n",
      "Epoch [88/300], Loss: 9.022651549506211e-10\n",
      "Epoch [89/300], Loss: 5.690395733434772e-07\n",
      "Epoch [90/300], Loss: 1.0362605524782964e-06\n",
      "Epoch [91/300], Loss: 2.4042631860399233e-07\n",
      "Epoch [92/300], Loss: 8.464412075692707e-07\n",
      "Epoch [93/300], Loss: 5.870853143630228e-07\n",
      "Epoch [94/300], Loss: 1.7853895837305345e-06\n",
      "Epoch [95/300], Loss: 1.0767005456102652e-06\n",
      "Epoch [96/300], Loss: 5.670273202085241e-07\n",
      "Epoch [97/300], Loss: 1.1715877679119302e-07\n",
      "Epoch [98/300], Loss: 2.2809518579469668e-08\n",
      "Epoch [99/300], Loss: 7.330963303631144e-07\n",
      "Epoch [100/300], Loss: 2.482615028831514e-07\n",
      "Epoch [101/300], Loss: 4.922648291945386e-08\n",
      "Epoch [102/300], Loss: 1.1575213079055915e-08\n",
      "Epoch [103/300], Loss: 4.163234544633451e-07\n",
      "Epoch [104/300], Loss: 2.125176479239599e-06\n",
      "Epoch [105/300], Loss: 1.3969644783173862e-06\n",
      "Epoch [106/300], Loss: 2.663968281524376e-06\n",
      "Epoch [107/300], Loss: 2.031049756823222e-06\n",
      "Epoch [108/300], Loss: 9.465249970475043e-07\n",
      "Epoch [109/300], Loss: 1.872795023505347e-07\n",
      "Epoch [110/300], Loss: 8.335546806126359e-07\n",
      "Epoch [111/300], Loss: 2.792612544474937e-07\n",
      "Epoch [112/300], Loss: 6.093712544286145e-08\n",
      "Epoch [113/300], Loss: 1.6449919731931217e-06\n",
      "Epoch [114/300], Loss: 2.3138858793458894e-06\n",
      "Epoch [115/300], Loss: 5.773064106762149e-07\n",
      "Epoch [116/300], Loss: 1.239207012559973e-07\n",
      "Epoch [117/300], Loss: 9.933124767158574e-07\n",
      "Epoch [118/300], Loss: 1.8630915099571155e-07\n",
      "Epoch [119/300], Loss: 3.9821039535259395e-08\n",
      "Epoch [120/300], Loss: 1.8002178898690602e-06\n",
      "Epoch [121/300], Loss: 5.126802273824893e-07\n",
      "Epoch [122/300], Loss: 1.165193927454844e-07\n",
      "Epoch [123/300], Loss: 9.035850113692234e-07\n",
      "Epoch [124/300], Loss: 3.255836015236824e-07\n",
      "Epoch [125/300], Loss: 6.287580677638571e-08\n",
      "Epoch [126/300], Loss: 8.784008997952952e-07\n",
      "Epoch [127/300], Loss: 3.3604475975501646e-07\n",
      "Epoch [128/300], Loss: 6.833433715408566e-08\n",
      "Epoch [129/300], Loss: 1.452254184096391e-08\n",
      "Epoch [130/300], Loss: 9.114012294864082e-07\n",
      "Epoch [131/300], Loss: 3.281735334592639e-07\n",
      "Epoch [132/300], Loss: 6.761135806776508e-08\n",
      "Epoch [133/300], Loss: 1.0247988193601287e-06\n",
      "Epoch [134/300], Loss: 1.0943367014237815e-06\n",
      "Epoch [135/300], Loss: 4.94058538969e-07\n",
      "Epoch [136/300], Loss: 6.567995294304296e-07\n",
      "Epoch [137/300], Loss: 6.38126234235592e-07\n",
      "Epoch [138/300], Loss: 1.2439648627848854e-07\n",
      "Epoch [139/300], Loss: 2.310356061845098e-08\n",
      "Epoch [140/300], Loss: 5.525158500640712e-09\n",
      "Epoch [141/300], Loss: 8.437847663182729e-07\n",
      "Epoch [142/300], Loss: 4.909496810201475e-07\n",
      "Epoch [143/300], Loss: 8.423613672564612e-08\n",
      "Epoch [144/300], Loss: 1.9331789033287528e-08\n",
      "Epoch [145/300], Loss: 4.014601811305318e-09\n",
      "Epoch [146/300], Loss: 7.426503746276077e-10\n",
      "Epoch [147/300], Loss: 1.6709150009393313e-10\n",
      "Epoch [148/300], Loss: 3.594677877310689e-11\n",
      "Epoch [149/300], Loss: 6.694139465599077e-12\n",
      "Epoch [150/300], Loss: 7.424489557933324e-11\n",
      "Epoch [151/300], Loss: 4.562677374253532e-07\n",
      "Epoch [152/300], Loss: 6.432724427019032e-07\n",
      "Epoch [153/300], Loss: 2.2222190454446178e-07\n",
      "Epoch [154/300], Loss: 4.4345866756501096e-08\n",
      "Epoch [155/300], Loss: 1.216286330332772e-08\n",
      "Epoch [156/300], Loss: 1.3211290212922222e-06\n",
      "Epoch [157/300], Loss: 7.631654703743607e-07\n",
      "Epoch [158/300], Loss: 1.176043269879301e-06\n",
      "Epoch [159/300], Loss: 2.9510142596222977e-07\n",
      "Epoch [160/300], Loss: 1.20399101533053e-06\n",
      "Epoch [161/300], Loss: 2.0928273336551229e-07\n",
      "Epoch [162/300], Loss: 4.348243409779684e-08\n",
      "Epoch [163/300], Loss: 8.978721321994954e-09\n",
      "Epoch [164/300], Loss: 1.874982856475693e-09\n",
      "Epoch [165/300], Loss: 3.5857307128378535e-07\n",
      "Epoch [166/300], Loss: 2.9930965056781944e-07\n",
      "Epoch [167/300], Loss: 6.262138896317992e-08\n",
      "Epoch [168/300], Loss: 6.090138106113396e-07\n",
      "Epoch [169/300], Loss: 6.00105572234888e-07\n",
      "Epoch [170/300], Loss: 1.1701148491111146e-07\n",
      "Epoch [171/300], Loss: 5.256833293665331e-07\n",
      "Epoch [172/300], Loss: 1.2950168759573444e-06\n",
      "Epoch [173/300], Loss: 9.059405963185441e-07\n",
      "Epoch [174/300], Loss: 3.035615665503144e-07\n",
      "Epoch [175/300], Loss: 1.8152914975644308e-06\n",
      "Epoch [176/300], Loss: 1.0587424732477757e-06\n",
      "Epoch [177/300], Loss: 8.064450833789039e-07\n",
      "Epoch [178/300], Loss: 1.348061372530296e-07\n",
      "Epoch [179/300], Loss: 3.0969175057737885e-08\n",
      "Epoch [180/300], Loss: 3.6669796860122617e-07\n",
      "Epoch [181/300], Loss: 3.272377215729483e-07\n",
      "Epoch [182/300], Loss: 9.581851683482245e-07\n",
      "Epoch [183/300], Loss: 1.4828768484242971e-06\n",
      "Epoch [184/300], Loss: 1.6263981033048935e-06\n",
      "Epoch [185/300], Loss: 1.8128982839371588e-06\n",
      "Epoch [186/300], Loss: 1.1250353715297479e-06\n",
      "Epoch [187/300], Loss: 8.9617825071997e-07\n",
      "Epoch [188/300], Loss: 6.712590749913261e-07\n",
      "Epoch [189/300], Loss: 1.232846000287502e-07\n",
      "Epoch [190/300], Loss: 4.481071542716819e-07\n",
      "Epoch [191/300], Loss: 3.6081943560750673e-07\n",
      "Epoch [192/300], Loss: 7.068761617712305e-08\n",
      "Epoch [193/300], Loss: 7.969815435382444e-08\n",
      "Epoch [194/300], Loss: 1.2728759095637088e-06\n",
      "Epoch [195/300], Loss: 2.2052758591462052e-07\n",
      "Epoch [196/300], Loss: 4.148847596863181e-07\n",
      "Epoch [197/300], Loss: 1.2896356316183244e-06\n",
      "Epoch [198/300], Loss: 2.110503436875133e-07\n",
      "Epoch [199/300], Loss: 8.573534224232127e-07\n",
      "Epoch [200/300], Loss: 1.6109631078009556e-06\n",
      "Epoch [201/300], Loss: 7.212298349656976e-07\n",
      "Epoch [202/300], Loss: 6.239431851184918e-07\n",
      "Epoch [203/300], Loss: 8.407781102448553e-07\n",
      "Epoch [204/300], Loss: 1.487950190282028e-07\n",
      "Epoch [205/300], Loss: 2.981865387940985e-08\n",
      "Epoch [206/300], Loss: 6.799890357733762e-08\n",
      "Epoch [207/300], Loss: 5.680393893037206e-07\n",
      "Epoch [208/300], Loss: 9.876017548204885e-07\n",
      "Epoch [209/300], Loss: 7.994748276729524e-07\n",
      "Epoch [210/300], Loss: 5.516791707904645e-07\n",
      "Epoch [211/300], Loss: 1.4906402587833156e-06\n",
      "Epoch [212/300], Loss: 1.226946592325362e-06\n",
      "Epoch [213/300], Loss: 2.2049197356821537e-07\n",
      "Epoch [214/300], Loss: 1.1292306472121538e-06\n",
      "Epoch [215/300], Loss: 5.278298200739329e-07\n",
      "Epoch [216/300], Loss: 9.84635834255787e-08\n",
      "Epoch [217/300], Loss: 1.0646221398763478e-06\n",
      "Epoch [218/300], Loss: 7.571064397637883e-07\n",
      "Epoch [219/300], Loss: 1.4349609789299222e-07\n",
      "Epoch [220/300], Loss: 2.8195019327950654e-08\n",
      "Epoch [221/300], Loss: 5.643938066901555e-09\n",
      "Epoch [222/300], Loss: 2.283583233099462e-09\n",
      "Epoch [223/300], Loss: 1.2781153770902165e-06\n",
      "Epoch [224/300], Loss: 1.1360077145639025e-06\n",
      "Epoch [225/300], Loss: 8.329322271904971e-07\n",
      "Epoch [226/300], Loss: 1.09060788489046e-06\n",
      "Epoch [227/300], Loss: 2.723401699711303e-07\n",
      "Epoch [228/300], Loss: 5.54620857240451e-07\n",
      "Epoch [229/300], Loss: 2.162533745853068e-06\n",
      "Epoch [230/300], Loss: 1.2915580480665767e-06\n",
      "Epoch [231/300], Loss: 3.716111960372359e-07\n",
      "Epoch [232/300], Loss: 7.860574724460889e-08\n",
      "Epoch [233/300], Loss: 1.653038228521808e-08\n",
      "Epoch [234/300], Loss: 2.322302282697919e-07\n",
      "Epoch [235/300], Loss: 3.139623626635668e-07\n",
      "Epoch [236/300], Loss: 1.021880011053966e-06\n",
      "Epoch [237/300], Loss: 3.5319212246531784e-07\n",
      "Epoch [238/300], Loss: 1.5432962054973132e-06\n",
      "Epoch [239/300], Loss: 1.4638432226377063e-06\n",
      "Epoch [240/300], Loss: 7.974510456421058e-07\n",
      "Epoch [241/300], Loss: 4.150834831406769e-07\n",
      "Epoch [242/300], Loss: 5.187039364695778e-07\n",
      "Epoch [243/300], Loss: 9.061886657724116e-08\n",
      "Epoch [244/300], Loss: 1.1032517667208808e-06\n",
      "Epoch [245/300], Loss: 7.974688331913171e-07\n",
      "Epoch [246/300], Loss: 8.393211148316482e-07\n",
      "Epoch [247/300], Loss: 4.3916315684011664e-07\n",
      "Epoch [248/300], Loss: 1.0130177369171989e-07\n",
      "Epoch [249/300], Loss: 8.610632105110483e-07\n",
      "Epoch [250/300], Loss: 1.1965000616953603e-06\n",
      "Epoch [251/300], Loss: 3.2160087570787255e-07\n",
      "Epoch [252/300], Loss: 4.2404911226867625e-07\n",
      "Epoch [253/300], Loss: 5.581028243994979e-07\n",
      "Epoch [254/300], Loss: 1.2421737932699628e-07\n",
      "Epoch [255/300], Loss: 3.8427193915779867e-07\n",
      "Epoch [256/300], Loss: 7.234514667331382e-07\n",
      "Epoch [257/300], Loss: 4.942499405302669e-07\n",
      "Epoch [258/300], Loss: 8.634971859766694e-08\n",
      "Epoch [259/300], Loss: 1.7986339286013564e-08\n",
      "Epoch [260/300], Loss: 5.668198516106671e-07\n",
      "Epoch [261/300], Loss: 5.500675150926781e-07\n",
      "Epoch [262/300], Loss: 1.2107413267514744e-06\n",
      "Epoch [263/300], Loss: 5.206168767202257e-07\n",
      "Epoch [264/300], Loss: 1.073748900903837e-07\n",
      "Epoch [265/300], Loss: 2.1720595366758744e-07\n",
      "Epoch [266/300], Loss: 1.4151564642350678e-06\n",
      "Epoch [267/300], Loss: 6.41192098527199e-07\n",
      "Epoch [268/300], Loss: 5.679290333571174e-07\n",
      "Epoch [269/300], Loss: 2.2755584017097874e-07\n",
      "Epoch [270/300], Loss: 8.158466586749569e-07\n",
      "Epoch [271/300], Loss: 2.227922070163757e-07\n",
      "Epoch [272/300], Loss: 1.3528819913233292e-06\n",
      "Epoch [273/300], Loss: 2.004329903826374e-06\n",
      "Epoch [274/300], Loss: 2.5469816797851763e-06\n",
      "Epoch [275/300], Loss: 1.1611640395869927e-06\n",
      "Epoch [276/300], Loss: 1.069424895305815e-06\n",
      "Epoch [277/300], Loss: 2.1095076574439986e-07\n",
      "Epoch [278/300], Loss: 4.175532247741032e-08\n",
      "Epoch [279/300], Loss: 9.004560930225836e-09\n",
      "Epoch [280/300], Loss: 2.9948749086239346e-07\n",
      "Epoch [281/300], Loss: 2.087364153191551e-07\n",
      "Epoch [282/300], Loss: 5.986928406503944e-08\n",
      "Epoch [283/300], Loss: 7.346662118656866e-07\n",
      "Epoch [284/300], Loss: 3.5573634549201927e-07\n",
      "Epoch [285/300], Loss: 1.0216286754349824e-06\n",
      "Epoch [286/300], Loss: 4.7272166492362544e-07\n",
      "Epoch [287/300], Loss: 2.582312309817869e-07\n",
      "Epoch [288/300], Loss: 6.534027530147313e-07\n",
      "Epoch [289/300], Loss: 5.005623942011539e-07\n",
      "Epoch [290/300], Loss: 8.526012145182449e-07\n",
      "Epoch [291/300], Loss: 2.953172262287751e-07\n",
      "Epoch [292/300], Loss: 3.6501642952657676e-07\n",
      "Epoch [293/300], Loss: 8.747476254455933e-07\n",
      "Epoch [294/300], Loss: 1.8737753892938969e-07\n",
      "Epoch [295/300], Loss: 7.494858795809023e-07\n",
      "Epoch [296/300], Loss: 1.5642289188377845e-06\n",
      "Epoch [297/300], Loss: 1.6021649287267792e-06\n",
      "Epoch [298/300], Loss: 9.040323831754904e-07\n",
      "Epoch [299/300], Loss: 7.140626205881517e-07\n",
      "Epoch [300/300], Loss: 1.6409268588368775e-07\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 75\n",
    "hidden_sizes = [32,16,8]\n",
    "output_y = 1\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "weight_decay = 1e-5\n",
    "\n",
    "\n",
    "# model\n",
    "model = NeuralNet(input_size, hidden_sizes, output_y).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_fn = nn.CrossE()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# data loader for iteration\n",
    "train_loader = torch.utils.data.DataLoader(dataset=df_train,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "# Train model\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # each iteration will give a batch of points \n",
    "    tot_avg_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(train_loader): \n",
    "        points = data[:, 0:75].clone().detach().float().to(device)\n",
    "        outputs = data[:,75].clone().detach().float().reshape(-1,1).to(device)\n",
    "\n",
    "        # forward prop\n",
    "        outputs_ = model.forward_propagation(points)\n",
    "        loss = loss_fn(outputs_, outputs)\n",
    "        \n",
    "        # Compute l2 loss component\n",
    "        l2_weight = 1.0\n",
    "        l2_parameters = []\n",
    "        for parameter in model.parameters():\n",
    "            l2_parameters.append(parameter.view(-1))\n",
    "        l2 = l2_weight * model.compute_l2_loss(torch.cat(l2_parameters))\n",
    "      \n",
    "        # Add l2 loss component\n",
    "        loss += l2\n",
    "\n",
    "        # backward prop and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tot_avg_loss += loss.item()\n",
    "\n",
    "    \n",
    "    tot_avg_loss /= train_loader.dataset.shape[0] \n",
    "    tot_avg_loss = torch.sqrt(torch.Tensor([tot_avg_loss])) \n",
    "    losses.append(tot_avg_loss.item())\n",
    "    \n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {tot_avg_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "# Plotting loss\n",
    "plt.figure()\n",
    "plt.plot(range(len(losses)), losses, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss over Iterations')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 76])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model and collect predictions\n",
    "pred_y = []\n",
    "ids = []\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "# data loader for iteration\n",
    "test_loader = torch.utils.data.DataLoader(dataset=df_test,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader): \n",
    "        iid = data[:,0].clone().detach().float().to(device)\n",
    "        points = data[:, 1:76].clone().detach().float().to(device)\n",
    "\n",
    "        outputs_ = model.forward_propagation(points)\n",
    "        \n",
    "        # Collect data for plotting\n",
    "        ids.append(iid.cpu())\n",
    "        pred_y.append(outputs_.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [item for tensor in ids for item in tensor.tolist()]\n",
    "pred_y = [item[0] for tensor in pred_y for item in tensor.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\"Id\": ids, \"SalePrice\": pred_y}\n",
    "res = pd.DataFrame(res)\n",
    "res.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
